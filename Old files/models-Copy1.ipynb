{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "text",
    "id": "kLXh3qYjBkIA"
   },
   "outputs": [],
   "source": [
    "#IPython extension to reload modules before executing user code.\n",
    "#'autoreload' reloads modules automatically before entering the execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gpustat\n",
    "\n",
    "stats = gpustat.GPUStatCollection.new_query()\n",
    "ids = map(lambda gpu: int(gpu.entry['index']), stats)\n",
    "ratios = map(lambda gpu: float(gpu.entry['memory.used'])/float(gpu.entry['memory.total']), stats)\n",
    "bestGPU = min(zip(ids, ratios), key=lambda x: x[1])[0]\n",
    "\n",
    "print(\"setGPU: Setting GPU to: {}\".format(bestGPU))\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(bestGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUPYhRcUNt_R"
   },
   "source": [
    "\n",
    "# MODEL:\n",
    "1 model normal\n",
    "1 model Learning rate /2 regulierement qui apporte une meilleure generalisation que directement un petit learning rate\n",
    "1 model avec weights decay #L² (qui ameliore nettement l'orthogonalisation et donc le generalisation pour les SGD #Simon'sArticle)\n",
    "\n",
    "(+effets positifs des petits minibatch sur la generalisation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtIGNMthNs6o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORT\n",
    "import numpy\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.datasets import cifar10 # we can use also cifar100\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "#print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2851,
     "status": "ok",
     "timestamp": 1544018175823,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "zeYCmklCneC2",
    "outputId": "15cf617f-01b7-495e-9272-c5e2b29c4484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "num_classes = 10\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "batch normalization apres chaque couche (depend de la taille du réseau)\n",
    "\n",
    "initialization:poids random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csHNHLbw2m5Q"
   },
   "outputs": [],
   "source": [
    "# MODEL 0\n",
    "def model_0(lr):\n",
    "  # create model\n",
    "  # https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  # Optimizer\n",
    "  sgd = optimizers.SGD(lr)\n",
    "  #opt = keras.optimizers.rmsprop(lr=0.001, decay=0.0)\n",
    "  # Compile model\n",
    "  #loss : 'categorical_crossentropy', 'mean_squared_error'\n",
    "  #optimizer : 'adam', 'sgd',adadelta, adagrad, RMSprop, Adamax, Nadam\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghIYG4xhnpSN"
   },
   "outputs": [],
   "source": [
    "# MODEL 1\n",
    "def model_1(lr, weightdecay):\n",
    "  # create model\n",
    "  # https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, kernel_regularizer=regularizers.l2(weightdecay)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes, kernel_regularizer=regularizers.l2(weightdecay)))\n",
    "  model.add(Activation('softmax'))\n",
    "  # Optimizer\n",
    "  sgd = optimizers.SGD(lr)\n",
    "  #opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "  # Compile model\n",
    "  #loss : 'categorical_crossentropy', 'mean_squared_error'\n",
    "  #optimizer : 'adam', 'sgd',adadelta, adagrad, RMSprop, Adamax, Nadam\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jhg6yjsnrR2"
   },
   "outputs": [],
   "source": [
    "# MODEL 2\n",
    "def model_2(lr, weightdecay):\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, kernel_regularizer=regularizers.l2(weightdecay)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes, kernel_regularizer=regularizers.l2(weightdecay)))\n",
    "  model.add(Activation('softmax'))\n",
    "  # Optimizer\n",
    "  sgd = optimizers.SGD(lr)\n",
    "  #opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "  # Compile model\n",
    "  #loss : 'categorical_crossentropy', 'mean_squared_error'\n",
    "  #optimizer : 'adam', 'sgd',adadelta, adagrad, RMSprop, Adamax, Nadam\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (11/2/2019) New models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True,\n",
    "                 use_bias = True,\n",
    "                 batchnorm_training = True,\n",
    "                 name = '',\n",
    "                 weight_decay=0.): #1e-4\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            activation-bn-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    regularizer = l2(weight_decay) if weight_decay >0. else None\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  use_bias = use_bias,\n",
    "                  name = name+'_conv',\n",
    "                  kernel_regularizer=regularizer)\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization(center = batchnorm_training, scale = batchnorm_training, name = name + '_batch')(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name = name+'_act')(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization(center = batchnorm_training, scale = batchnorm_training, name = name + '_batch')(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name = name+'_act')(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10, use_bias = True, batchnorm_training = True, weight_decay = 0.):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs, use_bias = use_bias, batchnorm_training = batchnorm_training, name = 'first',weight_decay = weight_decay)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides,\n",
    "                             use_bias = use_bias, batchnorm_training = batchnorm_training,\n",
    "                             name = str(stack)+'_'+str(res_block)+'_1',\n",
    "                             weight_decay = weight_decay)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None,\n",
    "                             use_bias = use_bias, batchnorm_training = batchnorm_training,\n",
    "                             name = str(stack)+'_'+str(res_block)+'_2',\n",
    "                             weight_decay = weight_decay)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False,\n",
    "                                 use_bias = use_bias, batchnorm_training = batchnorm_training,\n",
    "                                 name = str(stack)+'_'+str(res_block)+'_strided',\n",
    "                                 weight_decay = weight_decay)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias = use_bias,\n",
    "                    name = 'last_dense')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# VGG model from pytorchblog: http://torch.ch/blog/2015/07/30/cifar.html\n",
    "# Was also used by \"The marginal value of adaptive gradient methods in machine learning\"\n",
    "#==============================================================================\n",
    "\n",
    "def VGG_pytorchBlogStyle(input_shape, nbstages, nblayers, nbfilters,nbclasses,weight_decay=0., \n",
    "        kernel_constraint = None, kernel_initializer='glorot_uniform', include_top = True, use_batchnorm = True,\n",
    "        batchnorm_training = True, use_bias = True, act = 'relu', dropout = 0., kernel_size = (3,3),\n",
    "        batchnorm_position = 'before'):\n",
    "    '''\n",
    "    nbstages is the number of spatial dimension levels\n",
    "    nblayers is a list with nbstages elements containing the \n",
    "        number of convolutional layers per stage\n",
    "    nbfilters is a list of size sum(nbstages) with the \n",
    "        number of filters per convolutional layer in a stage\n",
    "    \n",
    "    kernel_constraint only applied on convolutional layers\n",
    "    \n",
    "    uses batchnorm after or before non-linearity\n",
    "    '''    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        if len(input_shape) == 2:\n",
    "            input_shape = input_shape + (3,)\n",
    "        channel_axis = -1\n",
    "    elif K.image_data_format() == 'channels_first':\n",
    "        if len(input_shape) == 2:\n",
    "            input_shape = (3,) + input_shape\n",
    "        channel_axis = 1\n",
    "    \n",
    "    if len(nblayers) != nbstages:\n",
    "        raise ValueError('nblayers should contain one element per stage.')\n",
    "    if len(nbfilters) != nbstages:\n",
    "        raise ValueError('nbfilters should contain one element per stage.')\n",
    "        \n",
    "    if batchnorm_position not in ['after','before']:\n",
    "        raise ValueError('batchnorm_position argument should be either \\'after\\' or \\'before\\'')\n",
    "    \n",
    "    regularizer = None\n",
    "    if weight_decay > 0.:\n",
    "        regularizer = l2(weight_decay)\n",
    "    \n",
    "    input_model = Input(shape = input_shape)\n",
    "    x = input_model\n",
    "    \n",
    "    layer_counter = 0\n",
    "    for s in range(nbstages):\n",
    "        for l in range(nblayers[s]):\n",
    "            x = Conv2D(nbfilters[s], kernel_size = kernel_size, padding = 'same',\n",
    "                       name = 'stage'+str(s)+'_layer'+str(l)+'_conv',\n",
    "                       kernel_constraint = kernel_constraint,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       kernel_regularizer=regularizer,\n",
    "                       use_bias = use_bias)(x)\n",
    "            \n",
    "            if use_batchnorm and batchnorm_position == 'before':\n",
    "                x = BatchNormalization(axis = channel_axis, name = 'stage'+str(s)+'_layer'+str(l)+'_batch',\n",
    "                                       center = batchnorm_training, scale = batchnorm_training)(x)\n",
    "                \n",
    "            if act is not 'leaky':\n",
    "                x = Activation('relu', name = 'stage'+str(s)+'_layer'+str(l)+'_relu')(x)\n",
    "            else:\n",
    "                x = LeakyReLU(alpha = 0.3, name = 'stage'+str(s)+'_layer'+str(l)+'_relu')(x)\n",
    "                \n",
    "            if use_batchnorm and batchnorm_position == 'after':\n",
    "                x = BatchNormalization(axis = channel_axis, name = 'stage'+str(s)+'_layer'+str(l)+'_batch',\n",
    "                                       center = batchnorm_training, scale = batchnorm_training)(x)\n",
    "            \n",
    "            if l<nblayers[s]-1:\n",
    "                if s == 0:\n",
    "                    x = Dropout(0.3)(x)\n",
    "                else:\n",
    "                    x = Dropout(0.4)(x)\n",
    "                        \n",
    "            layer_counter += 1\n",
    "        \n",
    "        x = MaxPooling2D((2,2),strides = (2,2), name = 'stage'+str(s)+'_pool')(x)\n",
    "    \n",
    "    if include_top:\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(512, kernel_initializer = kernel_initializer, use_bias = use_bias,kernel_regularizer=regularizer, name = 'dense1')(x)\n",
    "        if use_batchnorm and batchnorm_position == 'before':\n",
    "            x = BatchNormalization(axis = channel_axis,center = batchnorm_training, scale = batchnorm_training)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        if use_batchnorm and batchnorm_position == 'after':\n",
    "            x = BatchNormalization(axis = channel_axis,center = batchnorm_training, scale = batchnorm_training)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(nbclasses,name = 'last_dense', kernel_initializer = kernel_initializer, use_bias = use_bias,\n",
    "                 kernel_regularizer=regularizer)(x)\n",
    "        x = Activation('softmax',name = 'predictions')(x)\n",
    "    \n",
    "    return Model(input_model,x)\n",
    "    \n",
    "    \n",
    "def VGG_pytorchBlog(weight_decay=0.0005):\n",
    "    model = VGG_pytorchBlogStyle((32,32), 5, [2,2,3,3,3], [64,128,256,512,512],10,weight_decay=weight_decay,\n",
    "                                 batchnorm_training = False, use_bias = False, kernel_initializer='he_normal')\n",
    "    \n",
    "    weights_location = 'VGG_pytorchBlog_initial_weights.h5'\n",
    "    if not os.path.isfile(weights_location):\n",
    "        model.save_weights(weights_location)\n",
    "    else:\n",
    "        model.load_weights(weights_location)\n",
    "            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RX3K6_ov-_Db"
   },
   "outputs": [],
   "source": [
    "# pré-TRAINING\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "# build the models\n",
    "model0 = model_0(lr=0.1)\n",
    "model1 = model_1(lr=0.1, weightdecay=0.001)\n",
    "model2 = model_2(lr=0.1, weightdecay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "SVG(model_to_dot(model0).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights\n",
    "weights01 = model0.get_weights()\n",
    "weights11 = model1.get_weights()\n",
    "weights21 = model2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models and the initial weights\n",
    "model0.save(\"models/model0.h5\")\n",
    "model1.save(\"models/model1.h5\")\n",
    "model2.save(\"models/model2.h5\")\n",
    "model0.save_weights(\"models/model0_weights0.h5\")\n",
    "model1.save_weights(\"models/model1_weights0.h5\")\n",
    "model2.save_weights(\"models/model2_weights0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 740607,
     "status": "ok",
     "timestamp": 1544018913722,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "dTiWMuSw3Wl4",
    "outputId": "45eb2e89-a9d7-4944-9fc1-05934425aaae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model 0\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history0 = model0.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history0.history['acc'])\n",
    "plt.plot(history0.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history0.history['loss'])\n",
    "plt.plot(history0.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.save_weights(\"models/model0_weights25.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1285611,
     "status": "ok",
     "timestamp": 1544019458747,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "p5RKI8UontYP",
    "outputId": "a688dd9a-6277-4b53-ae34-ad796ecc7543",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model 1\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history1 = model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history1.history['acc'])\n",
    "plt.plot(history1.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_weights(\"models/model1_weights25.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if (epoch == 10) or (epoch == 20):\n",
    "        print(\"Changing learning rate \"+str(K.get_value(model2.optimizer.lr))+\" to \"+str(K.get_value(model2.optimizer.lr)/2))\n",
    "        K.set_value(model2.optimizer.lr, K.get_value(model2.optimizer.lr)/2)\n",
    "    return K.get_value(model2.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_lr = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1742189,
     "status": "ok",
     "timestamp": 1544019915343,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "UUjFN7HT1U4f",
    "outputId": "21b2c166-2dee-46f4-fdf7-687438347e15"
   },
   "outputs": [],
   "source": [
    "# Fit the model 2\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history2 = model2.fit(x_train, y_train, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs, \n",
    "               validation_data=(x_test, y_test), \n",
    "               shuffle=True,\n",
    "               callbacks=[change_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history2.history['acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_weights(\"models/model2_weights25.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1746657,
     "status": "ok",
     "timestamp": 1544019919828,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "tpKE6IpL8_k8",
    "outputId": "d1dd32e1-2786-454b-8817-bdda6e359441"
   },
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "# Final evaluation of the models\n",
    "scores0 = model0.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores0[0])\n",
    "print('Test accuracy:', scores0[1])\n",
    "print(\"Model 0 Error: %.2f%%\" % (100-scores0[1]*100))\n",
    "scores1 = model1.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores1[0])\n",
    "print('Test accuracy:', scores1[1])\n",
    "print(\"Model 1 CNN Error: %.2f%%\" % (100-scores1[1]*100))\n",
    "scores2 = model2.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores2[0])\n",
    "print('Test accuracy:', scores2[1])\n",
    "print(\"Model 2 Error: %.2f%%\" % (100-scores2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Analyse2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
