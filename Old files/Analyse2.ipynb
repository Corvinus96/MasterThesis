{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "text",
    "id": "kLXh3qYjBkIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gpustat\n",
    "\n",
    "stats = gpustat.GPUStatCollection.new_query()\n",
    "ids = map(lambda gpu: int(gpu.entry['index']), stats)\n",
    "ratios = map(lambda gpu: float(gpu.entry['memory.used'])/float(gpu.entry['memory.total']), stats)\n",
    "bestGPU = min(zip(ids, ratios), key=lambda x: x[1])[0]\n",
    "\n",
    "print(\"setGPU: Setting GPU to: {}\".format(bestGPU))\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(bestGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUPYhRcUNt_R"
   },
   "source": [
    "**Après l'entretien:**\n",
    "\n",
    "# MODEL:\n",
    "1 model normal\n",
    "1 model Learning rate /2 regulierement qui apporte une meilleure generalisation que directement un petit learning rate\n",
    "1 model avec weights decay #L² (qui ameliore nettement l'orthogonalisation et donc le generalisation pour les SGD #Simon'sArticle)\n",
    "\n",
    "(+effets positifs des petits minibatch sur la generalisation)\n",
    "\n",
    "# WEIGTHS\n",
    "Tester les deltas de poids pour une couche, mais aussi pour qq neurones seuls.\n",
    "Normaliser par rapport aux poids initiaux ou finaux.\n",
    "\n",
    "Regarder les poids qui changent le plus pour un neurone, et faire un histogramme pour plusieurs neurones. Voir si il y a un pique, ou plusieurs lorsqu'on generalise mieux ou pas.\n",
    "\n",
    "w_0 * w_t ~= 0 si orthogonaux (donc plus de chance de bien generaliser car tous les poids ont bougés de leur position initiale)\n",
    "\n",
    "# ACTIVATIONS\n",
    "Regarder l'information mutuelle AVEC DES INT #binaires (! restriction d'arrondi)\n",
    "Regarder la métrique spéciale de l'article pour comparer ce qu'apporte un neurone par rapport à tous les autres neurones de la couche (! restriction linéaire)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtIGNMthNs6o"
   },
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "import numpy\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.datasets import cifar10 # we can use also cifar100\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "#print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2851,
     "status": "ok",
     "timestamp": 1544018175823,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "zeYCmklCneC2",
    "outputId": "15cf617f-01b7-495e-9272-c5e2b29c4484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "num_classes = 10\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csHNHLbw2m5Q"
   },
   "outputs": [],
   "source": [
    "# MODEL 0\n",
    "def model_0():\n",
    "  # create model\n",
    "  # https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  # Optimizer\n",
    "  sgd = optimizers.SGD(lr=0.01)\n",
    "  #opt = keras.optimizers.rmsprop(lr=0.001, decay=0.0)\n",
    "  # Compile model\n",
    "  #loss : 'categorical_crossentropy', 'mean_squared_error'\n",
    "  #optimizer : 'adam', 'sgd',adadelta, adagrad, RMSprop, Adamax, Nadam\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghIYG4xhnpSN"
   },
   "outputs": [],
   "source": [
    "# MODEL 1\n",
    "def model_1():\n",
    "  # create model\n",
    "  # https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  # Optimizer\n",
    "  sgd = optimizers.SGD(lr=0.01)\n",
    "  #opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "  # Compile model\n",
    "  #loss : 'categorical_crossentropy', 'mean_squared_error'\n",
    "  #optimizer : 'adam', 'sgd',adadelta, adagrad, RMSprop, Adamax, Nadam\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jhg6yjsnrR2"
   },
   "outputs": [],
   "source": [
    "# MODEL 2\n",
    "def model_2():\n",
    "  # create model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(32, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Conv2D(64, (3, 3)))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(num_classes))\n",
    "  model.add(Activation('softmax'))\n",
    "  # Optimizer\n",
    "  sgd = optimizers.SGD(lr=0.02)\n",
    "  #opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "  # Compile model\n",
    "  #loss : 'categorical_crossentropy', 'mean_squared_error'\n",
    "  #optimizer : 'adam', 'sgd',adadelta, adagrad, RMSprop, Adamax, Nadam\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RX3K6_ov-_Db"
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "# build the models\n",
    "model0 = model_0()\n",
    "model1 = model_1()\n",
    "model2 = model_2()\n",
    "# get weights\n",
    "weights01 = model0.get_weights()\n",
    "weights11 = model1.get_weights()\n",
    "weights21 = model2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 740607,
     "status": "ok",
     "timestamp": 1544018913722,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "dTiWMuSw3Wl4",
    "outputId": "45eb2e89-a9d7-4944-9fc1-05934425aaae",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 243us/step - loss: 0.5821 - acc: 0.8018 - val_loss: 0.7067 - val_acc: 0.7621\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.5952 - acc: 0.7977 - val_loss: 0.7284 - val_acc: 0.7659\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.5896 - acc: 0.7995 - val_loss: 0.7054 - val_acc: 0.7719\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.5803 - acc: 0.8035 - val_loss: 0.7159 - val_acc: 0.7749\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.5705 - acc: 0.8083 - val_loss: 0.7361 - val_acc: 0.7666\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.5754 - acc: 0.8065 - val_loss: 0.7108 - val_acc: 0.7717\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.5849 - acc: 0.8018 - val_loss: 0.7354 - val_acc: 0.7574\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.5631 - acc: 0.8090 - val_loss: 0.7149 - val_acc: 0.7713\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.5758 - acc: 0.8037 - val_loss: 0.7231 - val_acc: 0.7646\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.5755 - acc: 0.8073 - val_loss: 0.7387 - val_acc: 0.7605\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.5688 - acc: 0.8087 - val_loss: 0.7573 - val_acc: 0.7542\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.5666 - acc: 0.8104 - val_loss: 0.7231 - val_acc: 0.7690\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.5580 - acc: 0.8124 - val_loss: 0.7446 - val_acc: 0.7645\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 12s 241us/step - loss: 0.5695 - acc: 0.8106 - val_loss: 0.7617 - val_acc: 0.7570\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.5659 - acc: 0.8107 - val_loss: 0.7164 - val_acc: 0.7800\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 12s 240us/step - loss: 0.5694 - acc: 0.8106 - val_loss: 0.7259 - val_acc: 0.7691\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.5804 - acc: 0.8059 - val_loss: 0.7482 - val_acc: 0.7655\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.5708 - acc: 0.8099 - val_loss: 0.7435 - val_acc: 0.7512\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.5781 - acc: 0.8092 - val_loss: 0.7299 - val_acc: 0.7642\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 12s 242us/step - loss: 0.5674 - acc: 0.8123 - val_loss: 0.7321 - val_acc: 0.7653\n"
     ]
    }
   ],
   "source": [
    "# Fit the model 0\n",
    "data_augmentation = False\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model0.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1285611,
     "status": "ok",
     "timestamp": 1544019458747,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "p5RKI8UontYP",
    "outputId": "a688dd9a-6277-4b53-ae34-ad796ecc7543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 2.1291 - acc: 0.2075 - val_loss: 1.9704 - val_acc: 0.3017\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 1.8232 - acc: 0.3382 - val_loss: 1.6276 - val_acc: 0.4162\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 1.6410 - acc: 0.4019 - val_loss: 1.5283 - val_acc: 0.4377\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 1.5233 - acc: 0.4469 - val_loss: 1.4398 - val_acc: 0.4875\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 1.4283 - acc: 0.4811 - val_loss: 1.3504 - val_acc: 0.5109\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 1.3554 - acc: 0.5117 - val_loss: 1.2507 - val_acc: 0.5604\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 1.2930 - acc: 0.5369 - val_loss: 1.2406 - val_acc: 0.5608\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.2339 - acc: 0.5573 - val_loss: 1.1510 - val_acc: 0.5880\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 1.1848 - acc: 0.5779 - val_loss: 1.0807 - val_acc: 0.6196\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 1.1383 - acc: 0.5946 - val_loss: 1.0700 - val_acc: 0.6183\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 1.0962 - acc: 0.6116 - val_loss: 1.0042 - val_acc: 0.6467\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 1.0590 - acc: 0.6254 - val_loss: 0.9845 - val_acc: 0.6524\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 1.0256 - acc: 0.6372 - val_loss: 0.9610 - val_acc: 0.6618\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 0.9922 - acc: 0.6499 - val_loss: 0.9316 - val_acc: 0.6730\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.9607 - acc: 0.6596 - val_loss: 0.9341 - val_acc: 0.6715\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.9369 - acc: 0.6715 - val_loss: 0.9095 - val_acc: 0.6841\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 0.9094 - acc: 0.6788 - val_loss: 0.8479 - val_acc: 0.7074\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.8871 - acc: 0.6881 - val_loss: 0.8531 - val_acc: 0.7029\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 0.8647 - acc: 0.6955 - val_loss: 0.8164 - val_acc: 0.7143\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 0.8433 - acc: 0.7020 - val_loss: 0.8479 - val_acc: 0.7023\n"
     ]
    }
   ],
   "source": [
    "# Fit the model 1\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch == 10:\n",
    "        print(\"Changing learning rate\")\n",
    "        K.set_value(model2.optimizer.lr, 0.01)\n",
    "    return K.get_value(model2.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_lr = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1742189,
     "status": "ok",
     "timestamp": 1544019915343,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "UUjFN7HT1U4f",
    "outputId": "21b2c166-2dee-46f4-fdf7-687438347e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 12s 244us/step - loss: 2.0052 - acc: 0.2627 - val_loss: 1.6887 - val_acc: 0.3953\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.6077 - acc: 0.4180 - val_loss: 1.5332 - val_acc: 0.4609\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 12s 238us/step - loss: 1.4024 - acc: 0.4932 - val_loss: 1.2447 - val_acc: 0.5524\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 1.2772 - acc: 0.5434 - val_loss: 1.1912 - val_acc: 0.5816\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 1.1928 - acc: 0.5759 - val_loss: 1.0623 - val_acc: 0.6271\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 1.1162 - acc: 0.6036 - val_loss: 1.0113 - val_acc: 0.6410\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 1.0538 - acc: 0.6285 - val_loss: 1.0028 - val_acc: 0.6485\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.9947 - acc: 0.6475 - val_loss: 0.9187 - val_acc: 0.6776\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.9500 - acc: 0.6654 - val_loss: 0.8578 - val_acc: 0.6979\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.8989 - acc: 0.6839 - val_loss: 0.8207 - val_acc: 0.7127\n",
      "Epoch 11/20\n",
      "Changing learning rate\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.8253 - acc: 0.7098 - val_loss: 0.7657 - val_acc: 0.7334\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.7978 - acc: 0.7176 - val_loss: 0.7546 - val_acc: 0.7357\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.7763 - acc: 0.7261 - val_loss: 0.7493 - val_acc: 0.7372\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.7539 - acc: 0.7353 - val_loss: 0.7552 - val_acc: 0.7373\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 12s 237us/step - loss: 0.7360 - acc: 0.7413 - val_loss: 0.7281 - val_acc: 0.7470\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.7180 - acc: 0.7468 - val_loss: 0.7020 - val_acc: 0.7521\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.7053 - acc: 0.7506 - val_loss: 0.6984 - val_acc: 0.7546\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.6894 - acc: 0.7562 - val_loss: 0.6963 - val_acc: 0.7536\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.6771 - acc: 0.7617 - val_loss: 0.6671 - val_acc: 0.7652\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.6591 - acc: 0.7692 - val_loss: 0.6643 - val_acc: 0.7687\n"
     ]
    }
   ],
   "source": [
    "# Fit the model 2\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model2.fit(x_train, y_train, \n",
    "               batch_size=batch_size, \n",
    "               epochs=epochs, \n",
    "               validation_data=(x_test, y_test), \n",
    "               shuffle=True,\n",
    "               callbacks=[change_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1746657,
     "status": "ok",
     "timestamp": 1544019919828,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "tpKE6IpL8_k8",
    "outputId": "d1dd32e1-2786-454b-8817-bdda6e359441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 110us/step\n",
      "Test loss: 2.3054959476470946\n",
      "Test accuracy: 0.1032\n",
      "Model 0 Error: 89.68%\n",
      "10000/10000 [==============================] - 1s 95us/step\n",
      "Test loss: 0.8478721293449402\n",
      "Test accuracy: 0.7023\n",
      "Model 1 CNN Error: 29.77%\n",
      "10000/10000 [==============================] - 1s 96us/step\n",
      "Test loss: 0.6642614018917083\n",
      "Test accuracy: 0.7687\n",
      "Model 2 Error: 23.13%\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION\n",
    "# Final evaluation of the models\n",
    "scores0 = model0.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores0[0])\n",
    "print('Test accuracy:', scores0[1])\n",
    "print(\"Model 0 Error: %.2f%%\" % (100-scores0[1]*100))\n",
    "scores1 = model1.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores1[0])\n",
    "print('Test accuracy:', scores1[1])\n",
    "print(\"Model 1 CNN Error: %.2f%%\" % (100-scores1[1]*100))\n",
    "scores2 = model2.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores2[0])\n",
    "print('Test accuracy:', scores2[1])\n",
    "print(\"Model 2 Error: %.2f%%\" % (100-scores2[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SorbI5TDN4cj"
   },
   "source": [
    "# WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pA4EX0UO79P2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://keras.io/layers/about-keras-layers/\n",
    "#print(weights)\n",
    "#print(weights[0][0][0][0])\n",
    "# print(weights12[10])\n",
    "#print(weights[11]) # last layer\n",
    "\n",
    "weights02 = model0.get_weights()\n",
    "weights12 = model1.get_weights()\n",
    "weights22 = model2.get_weights()\n",
    "\n",
    "weights02[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 964,
     "status": "error",
     "timestamp": 1544020296642,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "mAmUR6dpuGIL",
    "outputId": "1297dac0-aad1-4ce8-fafc-4750c81038a1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5944bfc551c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCOUCHE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mD_W0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights01\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOUCHE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweights02\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOUCHE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mD_W1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights11\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOUCHE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweights12\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOUCHE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mD_W2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights21\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOUCHE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweights22\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOUCHE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "# NORMALISATION\n",
    "from numpy import linalg\n",
    "\n",
    "COUCHE = 8\n",
    "\n",
    "D_W0 = numpy.absolute(weights01[COUCHE]-weights02[COUCHE])\n",
    "D_W1 = numpy.absolute(weights11[COUCHE]-weights12[COUCHE])\n",
    "D_W2 = numpy.absolute(weights21[COUCHE]-weights22[COUCHE])\n",
    "\n",
    "D_W0_norm = linalg.norm(D_W0)\n",
    "D_W1_norm = linalg.norm(D_W1) # ,np.inf)\n",
    "D_W2_norm = linalg.norm(D_W2)\n",
    "\n",
    "\n",
    "D_W0_normed = D_W0/D_W0_norm\n",
    "D_W1_normed = D_W1/D_W1_norm\n",
    "D_W2_normed = D_W2/D_W2_norm\n",
    "\n",
    "# another norm\n",
    "#weights11_norm2 = (weights11 - weights11.mean()) / (weights11.max() - weights11.min())\n",
    "#weights12_norm2 = (weights12 - weights12.mean()) / (weights12.max() - weights12.min())\n",
    "#weights21_norm2 = (weights21 - weights21.mean()) / (weights21.max() - weights21.min())\n",
    "#weights22_norm2 = (weights22 - weights22.mean()) / (weights22.max() - weights22.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1130,
     "status": "error",
     "timestamp": 1544020285828,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "o6ul1vYWxMuv",
    "outputId": "51fddfcd-7f61-4c6a-b2ed-4e3d5450d6ee"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a3df8969e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_W0_normed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# arguments are passed to np.histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Histogram D_W0_normed with 'auto' bins\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# PLOT\n",
    "# model0\n",
    "plt.hist(D_W0_normed, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram D_W0_normed with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2AMR7i53wgV"
   },
   "outputs": [],
   "source": [
    "#model1\n",
    "plt.hist(D_W1_normed, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram D_W1_normed with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ho99aTWD2Hb2"
   },
   "outputs": [],
   "source": [
    "#model2\n",
    "plt.hist(D_W2_normed, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Histogram D_W2_normed with 'auto' bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwTM0MdsN8IQ"
   },
   "source": [
    "# ACTIVATIONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-I7LXzj9-Au"
   },
   "outputs": [],
   "source": [
    "!pip install -q keract \n",
    "from keract import get_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbG_vuKF6sEJ"
   },
   "outputs": [],
   "source": [
    "# pour le test set, on regarde l'activation d'un echantillon de pairs de neurones (creation de X et Y)\n",
    "IM0 = get_activations(model0, [x_test[0], x_test[1], x_test[2], x_test[3], x_test[4], x_test[5], x_test[6], x_test[7], x_test[8], x_test[9], x_test[10], x_test[11], x_test[12], x_test[13]])\n",
    "print(IM0.keys())\n",
    "#IM1 = get_activations(model1, x_test)\n",
    "#IM2 = get_activations(model2, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvULjU5w-bGV"
   },
   "outputs": [],
   "source": [
    "#nameOfTheCouche = 'flatten_13/Reshape:0'\n",
    "nameOfTheCouche = 'dense_25/BiasAdd:0'\n",
    "numActivations = len(IM0[nameOfTheCouche])\n",
    "print('number of data used for activation : '+str(numActivations))\n",
    "numNeurons = len(IM0[nameOfTheCouche][0])\n",
    "print('number of neurons for the layer: '+str(numNeurons))\n",
    "#print(IM0['dense_2/BiasAdd:0'])\n",
    "plt.imshow(IM0[nameOfTheCouche])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0ygWfybGkzt"
   },
   "outputs": [],
   "source": [
    "# on selectionne des pair de neurones\n",
    "NombrePairs = 100\n",
    "NeuronsSelected = numpy.zeros((NombrePairs,2))\n",
    "for i in range(NombrePairs):\n",
    "  NeuronsSelected[i] = numpy.ceil(numpy.random.rand(2)*numNeurons)\n",
    "  while NeuronsSelected[i][0] == NeuronsSelected[i][1]:\n",
    "    NeuronsSelected[i] = numpy.ceil(numpy.random.rand(2)*numNeurons)\n",
    "#print(NeuronsSelected[56])\n",
    "#print(NeuronsSelected[56][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxVWKtj2ITPx"
   },
   "outputs": [],
   "source": [
    "MutualInfo = numpy.zeros((NombrePairs,1))\n",
    "for j in range(NombrePairs):\n",
    "  X = numpy.zeros((numActivations,1))\n",
    "  Y = numpy.zeros((numActivations,1))\n",
    "\n",
    "  i = 0\n",
    "  for activation in IM0[nameOfTheCouche]:\n",
    "    #print(activation)\n",
    "\n",
    "    X[i] = activation[int(NeuronsSelected[j][0])-1]\n",
    "    Y[i] = activation[int(NeuronsSelected[j][1])-1]\n",
    "    #print(X[i])\n",
    "    i = i+1\n",
    "\n",
    "  X = X.flatten()\n",
    "  Y = Y.flatten()\n",
    "  #print(X)\n",
    "  #print(Y)\n",
    "  MutualInfo[j] = sklearn.metrics.normalized_mutual_info_score(X,Y)\n",
    "  #print(MutualInfo[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLN1jQFcOjhv"
   },
   "outputs": [],
   "source": [
    "print(MutualInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsNnjFZ_Ezub"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linux/mnovak/Documents/Thesis/Code/myenv/lib/python3.5/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36400889786344465"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.normalized_mutual_info_score([1.1,1.0,1.0,1.0,0.0],[1.1,0.1,0.1,1.1,0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Analyse2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
