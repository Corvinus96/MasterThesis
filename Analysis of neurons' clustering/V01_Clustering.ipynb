{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython extension to reload modules before executing user code.\n",
    "#'autoreload' reloads modules automatically before entering the execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#set memory usage to 0.5\\nfrom keras.backend.tensorflow_backend import set_session\\nimport tensorflow as tf\\nconfig = tf.ConfigProto()\\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.33\\nset_session(tf.Session(config=config))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import gpustat\n",
    "\n",
    "#select the best free GPU on the nvidia card\n",
    "stats = gpustat.GPUStatCollection.new_query()\n",
    "ids = map(lambda gpu: int(gpu.entry['index']), stats)\n",
    "ratios = map(lambda gpu: float(gpu.entry['memory.used'])/float(gpu.entry['memory.total']), stats)\n",
    "bestGPU = min(zip(ids, ratios), key=lambda x: x[1])[0]\n",
    "bestGPU = 3\n",
    "\n",
    "print(\"setGPU: Setting GPU to: {}\".format(bestGPU))\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(bestGPU)\n",
    "\n",
    "'''\n",
    "#set memory usage to 0.5\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "set_session(tf.Session(config=config))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7754,
     "status": "ok",
     "timestamp": 1553629292171,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "WqsDDrUyOQbM",
    "outputId": "3a0b30e9-8797-4d08-b497-2ecd3eec3c59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q sklearn\n",
    "import collections\n",
    "import numpy as np, numpy\n",
    "from keract import get_activations, display_activations\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.datasets import cifar10, cifar100 # we can use also cifar100\n",
    "from keras.layers import Input, BatchNormalization, AveragePooling2D, ZeroPadding2D, LeakyReLU, GlobalAveragePooling2D, Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sys\n",
    "#sys.executable\n",
    "#sys.path\n",
    "import time\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbtqLzrROuSj"
   },
   "outputs": [],
   "source": [
    "def normalize(x, mean, std):\n",
    "    # This function normalizes inputs for zero mean and unit variance to speed up learning.\n",
    "    \n",
    "    # In case std = 0, we add eps = 1e-7\n",
    "    eps = K.epsilon()\n",
    "    x = (x-mean)/(std+eps)\n",
    "    return x\n",
    "  \n",
    "def import_cifar(dataset):\n",
    "    if dataset == 10:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    elif dataset == 100:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    # By default, they are uint8 but we need them float to normalize them\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # Calculating the mean and standard deviation of the training data\n",
    "    mean = np.mean(x_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(x_train, axis=(0, 1, 2, 3))\n",
    "    \n",
    "    # Normalizing \n",
    "    x_train = normalize(x_train, mean, std)\n",
    "    x_test = normalize(x_test, mean, std)\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes=dataset)\n",
    "    y_test = to_categorical(y_test,  num_classes=dataset)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 254523,
     "status": "ok",
     "timestamp": 1553629538981,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "WV_iymO6PAJP",
    "outputId": "c9f4bcdb-1ba5-4402-ac75-21cb49868b08"
   },
   "outputs": [],
   "source": [
    "# LOAD DATABase\n",
    "num_classes = 10\n",
    "(x_train, y_train), (x_test, y_test) = import_cifar(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trCmD1WVQlEv"
   },
   "outputs": [],
   "source": [
    "# Architecture taken from https://github.com/geifmany/cifar-vgg\n",
    "# Weight decay and Dropout have been removed\n",
    "# BatchNormalization before activations\n",
    "def VGG16_Vanilla_beta(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        #0\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #3\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #7\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #10\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #14\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #17\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #20\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #24\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #27\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #30\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #34\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #37\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #40\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        #45\n",
    "        Dense(512),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #48\n",
    "        Dense(num_classes),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('softmax')])\n",
    "    return model\n",
    "\n",
    "# Architecture taken from https://github.com/geifmany/cifar-vgg\n",
    "# BatchNormalization before activations\n",
    "def VGG16_beta(input_shape, num_classes, weight_decay):\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.3),\n",
    "        Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(512, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.5),\n",
    "        Dense(num_classes, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('softmax')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'modelLAY_65.49%test_94%train_epoch50_0012lr.hdf5'\n",
    "#model_name = 'modelLAY_75.47%test_87.42%train_epoch50_0018lr.hdf5'\n",
    "#model_name = 'modelLAY_80.85%test_96.694%train_epoch50_0008lr'\n",
    "#model_name = 'modelSGD_71.7%test_100%train_epoch21_0.002lr'\n",
    "#model_name = 'modelSGD_76.67%test_100%train_epoch39_0.004lr'\n",
    "#model_name = 'modelSGD_80.92%test_99.994%train_epoch50_0.008lr'\n",
    "#model_name = 'modelWD_63.63%test_91.126%train_epoch50_0.014lr_0005'\n",
    "#model_name = 'modelWD_75.58%test_97.606%train_epoch50_0.002lr_0005'\n",
    "#model_name = 'modelWD_76.06%test_94.556%train_epoch50_0.008lr_0005'\n",
    "\n",
    "modelename = 'vgg16_sgd_1'\n",
    "#layca, sgd, wdecay\n",
    "model_name = '../weights/4/{}/final/weights-final.hdf5'.format(modelename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 345320,
     "status": "ok",
     "timestamp": 1553629629855,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "t9gnffFxQ8JJ",
    "outputId": "88226a4f-612d-4a6a-f3fb-facb256c5034"
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL SDG/LAYCA or WD\n",
    "model = VGG16_Vanilla_beta(input_shape=(32,32,3), num_classes=10)\n",
    "#model = VGG16_beta(input_shape=(32,32,3), num_classes=10, weight_decay=0.005)\n",
    "\n",
    "#model.load_weights('weights/{}.hdf5'.format(model_name))\n",
    "model.load_weights('{}'.format(model_name))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.002, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 377703,
     "status": "ok",
     "timestamp": 1553629662262,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "F3CL2zkoRGex",
    "outputId": "0b0eb53d-3225-4afa-b143-f474bffd9074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 505us/step\n",
      "Test loss: 0.9577856213569641\n",
      "Test accuracy: 0.7507\n",
      "24.93% : Model Error\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION\n",
    "# Final evaluation of the models\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print(\"%.2f%% : Model Error\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAGlHxHIRLJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['activation_11/Relu:0', 'max_pooling2d_2/MaxPool:0', 'activation_10/Relu:0', 'activation_5/Relu:0', 'batch_normalization_10/cond/Merge:0', 'batch_normalization_8/cond/Merge:0', 'conv2d_5/BiasAdd:0', 'conv2d_2/BiasAdd:0', 'batch_normalization_12/cond/Merge:0', 'conv2d_8/BiasAdd:0', 'batch_normalization_4/cond/Merge:0', 'batch_normalization_9/cond/Merge:0', 'activation_6/Relu:0', 'activation_1/Relu:0', 'activation_3/Relu:0', 'activation_9/Relu:0', 'conv2d_12/BiasAdd:0', 'batch_normalization_3/cond/Merge:0', 'conv2d_10/BiasAdd:0', 'conv2d_9/BiasAdd:0', 'flatten_1/Reshape:0', 'max_pooling2d_4/MaxPool:0', 'activation_14/Relu:0', 'activation_15/Softmax:0', 'conv2d_6/BiasAdd:0', 'max_pooling2d_1/MaxPool:0', 'activation_13/Relu:0', 'dense_2/BiasAdd:0', 'batch_normalization_14/cond/Merge:0', 'activation_2/Relu:0', 'conv2d_11/BiasAdd:0', 'conv2d_13/BiasAdd:0', 'activation_8/Relu:0', 'max_pooling2d_3/MaxPool:0', 'conv2d_7/BiasAdd:0', 'activation_12/Relu:0', 'batch_normalization_2/cond/Merge:0', 'activation_7/Relu:0', 'conv2d_1/BiasAdd:0', 'batch_normalization_6/cond/Merge:0', 'dense_1/BiasAdd:0', 'activation_4/Relu:0', 'batch_normalization_11/cond/Merge:0', 'max_pooling2d_5/MaxPool:0', 'batch_normalization_1/cond/Merge:0', 'batch_normalization_15/cond/Merge:0', 'conv2d_4/BiasAdd:0', 'conv2d_3/BiasAdd:0', 'batch_normalization_13/cond/Merge:0', 'batch_normalization_5/cond/Merge:0', 'batch_normalization_7/cond/Merge:0'])\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               1024      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 14,991,966\n",
      "Trainable params: 14,982,474\n",
      "Non-trainable params: 9,492\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# On regarde l'activation des pairs de neurones pour un echantillon test pris au hasard\n",
    "Echantillon = []\n",
    "nombreDImagesDActivation = 1000\n",
    "choix = np.random.choice(x_test.shape[0], nombreDImagesDActivation)\n",
    "Echantillon = x_train[choix, :, : , :]\n",
    "\n",
    "IM = get_activations(model, Echantillon)\n",
    "# on affiche les noms des differentes couches\n",
    "print(IM.keys())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take 100% of the cluster (could take less if the clusters are more)\n",
    "# we suppose k = 2 (?!)\n",
    "# DO : ratio in SilhouetteScore\n",
    "# SEE LATER...\n",
    "\n",
    "def AverageDistance(vecteur,vecteurs_without_vecteur):\n",
    "    nCluster_i = len(vecteurs_without_vecteur)\n",
    "    distance = 0\n",
    "    for vect in vecteurs_without_vecteur:\n",
    "        #distance = distance + scipy.spatial.distance.euclidean(vecteur,vect)\n",
    "        distance = distance + numpy.linalg.norm(vecteur-vect)\n",
    "    a = (1/(nCluster_i))*distance\n",
    "    return a\n",
    "\n",
    "def AverageDissimilarity(vecteur, vecteurs_from_another_cluster):\n",
    "    nCluster_j = len(vecteurs_from_another_cluster)\n",
    "    distance = 0\n",
    "    for vect in vecteurs_from_another_cluster:\n",
    "        #distance = distance + scipy.spatial.distance.euclidean(vecteur,vect)\n",
    "        distance = distance + numpy.linalg.norm(vecteur-vect)\n",
    "    b = (1/nCluster_j)*distance\n",
    "    return b\n",
    "\n",
    "def Silhouette(vecteurs, a, b):\n",
    "    s = (b-a)/(max(a,b))\n",
    "    return s\n",
    "\n",
    "def SilhouetteScore(vecteurs_0,vecteurs_1):\n",
    "    if len(vecteurs_0) == None or len(vecteurs_1) == None or len(vecteurs_0) == 0 or len(vecteurs_1) == 0:\n",
    "        print('Error: all entries actived or not: Probably a pathologique neuron')\n",
    "        return -2\n",
    "    \n",
    "    SilhouetteScoreSum = 0\n",
    "    \n",
    "    if len(vecteurs_0) < 3 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_0)):\n",
    "            a = AverageDistance(vecteurs_0[i], vecteurs_0[0:i]+vecteurs_0[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_0[i], vecteurs_1)\n",
    "            s = Silhouette(vecteurs_0, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "    \n",
    "    if len(vecteurs_1) < 2 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_1)):\n",
    "            a = AverageDistance(vecteurs_1[i], vecteurs_1[0:i]+vecteurs_1[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_1[i], vecteurs_0)\n",
    "            s = Silhouette(vecteurs_1, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "\n",
    "    SilhouetteScore = SilhouetteScoreSum/(len(vecteurs_0)+len(vecteurs_1))\n",
    "        \n",
    "    return SilhouetteScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNUSED\n",
    "def SilhouetteScoreRatio(vecteurs_0,vecteurs_1, ratio):\n",
    "    # RATIO UNUSED\n",
    "    if len(vecteurs_0) == None or len(vecteurs_1) == None or len(vecteurs_0) == 0 or len(vecteurs_1) == 0:\n",
    "        print('Error: all entries actived or not: Probably a pathologique neuron')\n",
    "        return -2\n",
    "    \n",
    "    SilhouetteScoreSum = 0\n",
    "    \n",
    "    if len(vecteurs_0) < 3 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_0)):\n",
    "            a = AverageDistance(vecteurs_0[i], vecteurs_0[0:i]+vecteurs_0[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_0[i], vecteurs_1)\n",
    "            s = Silhouette(vecteurs_0, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "    \n",
    "    if len(vecteurs_1) < 2 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_1)):\n",
    "            a = AverageDistance(vecteurs_1[i], vecteurs_1[0:i]+vecteurs_1[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_1[i], vecteurs_0)\n",
    "            s = Silhouette(vecteurs_1, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "\n",
    "    SilhouetteScore = SilhouetteScoreSum/(len(vecteurs_0)+len(vecteurs_1))\n",
    "        \n",
    "    return SilhouetteScore\n",
    "\n",
    "def SilhouetteScoreNormalized(vecteurs_0,vecteurs_1,Weightsratio): #, ratio):\n",
    "    \n",
    "    if len(vecteurs_0) == None or len(vecteurs_1) == None or len(vecteurs_0) == 0 or len(vecteurs_1) == 0:\n",
    "        print('Error: all entries actived or not: Probably a pathologique neuron')\n",
    "        return -2\n",
    "    \n",
    "    SilhouetteScoreSum = 0\n",
    "    \n",
    "    if len(vecteurs_0) < 3 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_0)):\n",
    "            a = AverageDistance(vecteurs_0[i], vecteurs_0[0:i]+vecteurs_0[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_0[i], vecteurs_1)\n",
    "            s = Silhouette(vecteurs_0, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "    \n",
    "    if len(vecteurs_1) == 1 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_1)):\n",
    "            a = AverageDistance(vecteurs_1[i], vecteurs_1[0:i]+vecteurs_1[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_1[i], vecteurs_0)\n",
    "            s = Silhouette(vecteurs_1, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "\n",
    "    SilhouetteScore = SilhouetteScoreSum/(len(vecteurs_0)+len(vecteurs_1))*Weightsratio\n",
    "        \n",
    "    return SilhouetteScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pour tout le modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: all entries actived or not: Probably a pathologique neuron\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-82ed17678416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSilhouetteScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVecteurs_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVecteurs_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mS2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVecteurs_All\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabels_All\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# -> give the same result ! :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/linux/mnovak/penv/lib/python3.5/site-packages/sklearn/metrics/cluster/unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/linux/mnovak/penv/lib/python3.5/site-packages/sklearn/metrics/cluster/unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mlabel_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mcheck_number_of_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/linux/mnovak/penv/lib/python3.5/site-packages/sklearn/metrics/cluster/unsupervised.py\u001b[0m in \u001b[0;36mcheck_number_of_labels\u001b[0;34m(n_labels, n_samples)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_labels\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         raise ValueError(\"Number of labels is %d. Valid values are 2 \"\n\u001b[0;32m---> 35\u001b[0;31m                          \"to n_samples - 1 (inclusive)\" % n_labels)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "Silhouette_Score_Mean = []\n",
    "\n",
    "NeuronsToTest = 10\n",
    "\n",
    "for i in range(12):\n",
    "    #Layer to test\n",
    "    couche_name_1 = 'activation_{}/Relu:0'.format(i+1)\n",
    "    couche_name_2 = 'conv2d_{}/BiasAdd:0'.format(i+2)\n",
    "    couche_name_3 = 'activation_{}/Relu:0'.format(i+2)\n",
    "    \n",
    "    NeuronsToTestIntoTheLayer = NeuronsToTest\n",
    "\n",
    "    X_Kernel_size = len(IM[couche_name_2][0,:,:,:])\n",
    "    Y_Kernel_size = len(IM[couche_name_2][0,0,:,:])\n",
    "    Z_Kernel_size = len(IM[couche_name_2][0,0,0,:])\n",
    "\n",
    "    Silhouette_Score_Sum = 0\n",
    "\n",
    "    for i in range(NeuronsToTestIntoTheLayer):\n",
    "        #Choix du neurone\n",
    "        X = int(numpy.ceil(numpy.random.rand(1)*X_Kernel_size)-1)\n",
    "        Y = int(numpy.ceil(numpy.random.rand(1)*Y_Kernel_size)-1)\n",
    "        Z = int(numpy.ceil(numpy.random.rand(1)*Z_Kernel_size)-1)\n",
    "        #specification de ses bornes\n",
    "        X_inf = X-1\n",
    "        Y_inf = Y-1\n",
    "        X_sup = X+1\n",
    "        Y_sup = Y+1\n",
    "        if X == 0: X_inf = 0\n",
    "        if Y == 0: Y_inf = 0   \n",
    "        if X == X_Kernel_size-1: X_sup = X_Kernel_size-1\n",
    "        if Y == Y_Kernel_size-1: Y_sup = Y_Kernel_size-1\n",
    "\n",
    "        Vecteurs_0 = [] #inactivé\n",
    "        Vecteurs_1 = [] #activé\n",
    "        ##Vecteurs_All = []\n",
    "        ##Labels_All = []\n",
    "        \n",
    "        for j in range(nombreDImagesDActivation):\n",
    "            if IM[couche_name_3][j,X,Y,Z] == 0.0:\n",
    "                Vecteurs_0.append(IM[couche_name_1][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                ##Vecteurs_All.append(IM[couche_name_1][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                ##Labels_All.append(0)\n",
    "            elif IM[couche_name_3][j,X,Y,Z] > 0.0:\n",
    "                Vecteurs_1.append(IM[couche_name_1][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                ##Vecteurs_All.append(IM[couche_name_1][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                ##Labels_All.append(1)\n",
    "            else:\n",
    "                print('error !')\n",
    "                break\n",
    "        S = SilhouetteScore(Vecteurs_0,Vecteurs_1)\n",
    "        ##S2 = sklearn.metrics.silhouette_score(Vecteurs_All, Labels_All, metric='euclidean') # -> give the same result ! :)\n",
    "        \n",
    "        if S != -2:\n",
    "            #print('Neuron '+str(i+1)+'/'+str(NeuronsToTestIntoTheLayer)+' has a silhouette score of '+str(S))\n",
    "            Silhouette_Score_Sum = Silhouette_Score_Sum + S\n",
    "        else:\n",
    "            NeuronsToTestIntoTheLayer = NeuronsToTestIntoTheLayer -1\n",
    "\n",
    "    Silhouette_Score_Mean.append(Silhouette_Score_Sum / NeuronsToTestIntoTheLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Silhouette_Score_Mean)\n",
    "print(np.mean(Silhouette_Score_Mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot:\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "#plt.ylim(top=0.1)\n",
    "nombreCouches = len(Silhouette_Score_Mean)\n",
    "x = range(nombreCouches)\n",
    "\n",
    "markerline, stemlines, baseline = plt.stem(x, Silhouette_Score_Mean, markerfmt='o', label='model ')\n",
    "plt.setp(stemlines, 'color', plt.getp(markerline,'color'))\n",
    "plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "plt.plot(np.mean(Silhouette_Score_Mean)*np.ones((nombreCouches,1)))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Couches')\n",
    "plt.xticks(np.arange(nombreCouches), ('Conv 1','Conv 2','Conv 3','Conv 4','Conv 5','Conv 6','Conv 7','Conv 8','Conv 9','Conv10','Conv11','Conv12','Conv13','Dense1','Dense2'), rotation=60)\n",
    "plt.ylabel('Silhouette score mean')\n",
    "plt.title('Silhouette score mean by layer for the model '+str(modelename)+' for '+str(nombreDImagesDActivation)+' activation images and '+str(NeuronsToTest)+ ' neurons')\n",
    "plt.show()\n",
    "fig.savefig('figures/SilhouetteScoreMean_{}.png'.format(modelename), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "V5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
