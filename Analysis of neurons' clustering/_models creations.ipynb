{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#IPython extension to reload modules before executing user code.\n",
    "#'autoreload' reloads modules automatically before entering the execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#set memory usage to 0.5\\nfrom keras.backend.tensorflow_backend import set_session\\nimport tensorflow as tf\\nconfig = tf.ConfigProto()\\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.33\\nset_session(tf.Session(config=config))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import gpustat\n",
    "\n",
    "#select the best free GPU on the nvidia card\n",
    "stats = gpustat.GPUStatCollection.new_query()\n",
    "ids = map(lambda gpu: int(gpu.entry['index']), stats)\n",
    "ratios = map(lambda gpu: float(gpu.entry['memory.used'])/float(gpu.entry['memory.total']), stats)\n",
    "bestGPU = min(zip(ids, ratios), key=lambda x: x[1])[0]\n",
    "bestGPU = 1\n",
    "\n",
    "print(\"setGPU: Setting GPU to: {}\".format(bestGPU))\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(bestGPU)\n",
    "\n",
    "'''\n",
    "#set memory usage to 0.5\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "set_session(tf.Session(config=config))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q sklearn\n",
    "import collections\n",
    "import numpy as np, numpy\n",
    "from keract import get_activations, display_activations\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.datasets import cifar10, cifar100 # we can use also cifar100\n",
    "from keras.layers import Input, BatchNormalization, AveragePooling2D, ZeroPadding2D, LeakyReLU, GlobalAveragePooling2D, Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sys\n",
    "#sys.executable\n",
    "#sys.path\n",
    "import time\n",
    "from IPython.display import SVG\n",
    "\n",
    "from layca_optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(x, mean, std):\n",
    "    # This function normalizes inputs for zero mean and unit variance to speed up learning.\n",
    "    \n",
    "    # In case std = 0, we add eps = 1e-7\n",
    "    eps = K.epsilon()\n",
    "    x = (x-mean)/(std+eps)\n",
    "    return x\n",
    "  \n",
    "def import_cifar(dataset):\n",
    "    if dataset == 10:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    elif dataset == 100:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    # By default, they are uint8 but we need them float to normalize them\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # Calculating the mean and standard deviation of the training data\n",
    "    mean = np.mean(x_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(x_train, axis=(0, 1, 2, 3))\n",
    "    \n",
    "    # Normalizing \n",
    "    x_train = normalize(x_train, mean, std)\n",
    "    x_test = normalize(x_test, mean, std)\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes=dataset)\n",
    "    y_test = to_categorical(y_test,  num_classes=dataset)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATABase\n",
    "num_classes = 10\n",
    "(x_train, y_train), (x_test, y_test) = import_cifar(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Architecture taken from https://github.com/geifmany/cifar-vgg\n",
    "# Weight decay and Dropout have been removed\n",
    "# BatchNormalization before activations\n",
    "def VGG16_Vanilla_beta(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        #0\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #3\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #7\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #10\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #14\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #17\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #20\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #24\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #27\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #30\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #34\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #37\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #40\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        #45\n",
    "        Dense(512),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #48\n",
    "        Dense(num_classes),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('softmax')])\n",
    "    return model\n",
    "\n",
    "# Architecture taken from https://github.com/geifmany/cifar-vgg\n",
    "# BatchNormalization before activations\n",
    "def VGG16_beta(input_shape, num_classes, weight_decay):\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.3),\n",
    "        Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(512, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.5),\n",
    "        Dense(num_classes, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('softmax')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the models\n",
    "modelLAY = VGG16_Vanilla_beta(input_shape=(32,32,3), num_classes=10) #without Weight Decay but batchNorm before activation\n",
    "modelSGD = VGG16_Vanilla_beta(input_shape=(32,32,3), num_classes=10) #without Weight Decay but batchNorm before activation\n",
    "modelWD = VGG16_beta(input_shape=(32,32,3), num_classes=10, weight_decay=0.005) #with Weight Decay, and batchNorm before activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##OPTIONNAL\n",
    "# get the initials weights\n",
    "#weightsSGD_init = modelSGD.get_weights()\n",
    "#weightsWD_init = modelWD.get_weights()\n",
    "#weightsLAY_init = modelLAY.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.0045, momentum=0.9, nesterov=True, layca=True)\n",
    "modelLAY.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimization details\n",
    "sgd = SGD(lr=0.0045, momentum=0.9, nesterov=True, layca=False)\n",
    "modelSGD.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.002, momentum=0.9, nesterov=True, layca=False)\n",
    "modelWD.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLAY.save_weights('weights/modelLAY_weights_init.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelSGD.save_weights('weights/modelSGD_weights_init.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWD.save_weights('weights/modelWD_weights_init.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1    acc :  58.849999996185296    val_acc :  66.96\n",
      "epoch :  2    acc :  76.5180000038147    val_acc :  71.54\n",
      "epoch :  3    acc :  82.80799999809265    val_acc :  74.63\n",
      "epoch :  4    acc :  86.63800000190734    val_acc :  77.05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    fit = modelLAY.fit(x_train, y_train, batch_size=256, epochs=1, validation_data=(x_test, y_test), shuffle=True, verbose=0)\n",
    "    print('epoch : ',str(epoch+1), '   acc : ',str(fit.history['acc'][0]*100), '   val_acc : ',str(fit.history['val_acc'][0]*100))\n",
    "    if fit.history['acc'][0] >= 1. :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    fit = modelSGD.fit(x_train, y_train, batch_size=256, epochs=1, validation_data=(x_test, y_test), shuffle=True, verbose=0)\n",
    "    print('epoch : ',str(epoch+1), '   acc : ',str(fit.history['acc'][0]*100), '   val_acc : ',str(fit.history['val_acc'][0]*100))\n",
    "    if fit.history['acc'][0] >= 1. :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    fit = modelWD.fit(x_train, y_train, batch_size=256, epochs=1, validation_data=(x_test, y_test), shuffle=True, verbose=0)\n",
    "    print('epoch : ',str(epoch+1), '   acc : ',str(fit.history['acc'][0]*100), '   val_acc : ',str(fit.history['val_acc'][0]*100))\n",
    "    if fit.history['acc'][0] >= 1. :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n LAYCA models:')\n",
    "\n",
    "scores = modelLAY.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EVALUATION\n",
    "# Final evaluation of the models\n",
    "\n",
    "print('\\n SGD models:')\n",
    "\n",
    "scores = modelSGD.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n Weight Decay models:')\n",
    "\n",
    "scores = modelWD.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model by model\n",
    "modelLAY.save_weights('weights/modelLAY_%test_%train_epoch_0.0045lr.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSGD.save_weights('weights/modelSGD_%test_%train_epoch_0.0045lr.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWD.save_weights('weights/modelWD_%test_%train_epoch_0.002lr_0005.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
