{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython extension to reload modules before executing user code.\n",
    "#'autoreload' reloads modules automatically before entering the execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#set memory usage to 0.5\\nfrom keras.backend.tensorflow_backend import set_session\\nimport tensorflow as tf\\nconfig = tf.ConfigProto()\\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.33\\nset_session(tf.Session(config=config))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import gpustat\n",
    "\n",
    "#select the best free GPU on the nvidia card\n",
    "stats = gpustat.GPUStatCollection.new_query()\n",
    "ids = map(lambda gpu: int(gpu.entry['index']), stats)\n",
    "ratios = map(lambda gpu: float(gpu.entry['memory.used'])/float(gpu.entry['memory.total']), stats)\n",
    "bestGPU = min(zip(ids, ratios), key=lambda x: x[1])[0]\n",
    "bestGPU = 3\n",
    "\n",
    "print(\"setGPU: Setting GPU to: {}\".format(bestGPU))\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(bestGPU)\n",
    "\n",
    "'''\n",
    "#set memory usage to 0.5\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "set_session(tf.Session(config=config))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7754,
     "status": "ok",
     "timestamp": 1553629292171,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "WqsDDrUyOQbM",
    "outputId": "3a0b30e9-8797-4d08-b497-2ecd3eec3c59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q sklearn\n",
    "import collections\n",
    "import numpy as np, numpy\n",
    "from keract import get_activations, display_activations\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.datasets import cifar10, cifar100 # we can use also cifar100\n",
    "from keras.layers import Input, BatchNormalization, AveragePooling2D, ZeroPadding2D, LeakyReLU, GlobalAveragePooling2D, Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import operator \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sys\n",
    "#sys.executable\n",
    "#sys.path\n",
    "import time\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbtqLzrROuSj"
   },
   "outputs": [],
   "source": [
    "def normalize(x, mean, std):\n",
    "    # This function normalizes inputs for zero mean and unit variance to speed up learning.\n",
    "    \n",
    "    # In case std = 0, we add eps = 1e-7\n",
    "    eps = K.epsilon()\n",
    "    x = (x-mean)/(std+eps)\n",
    "    return x\n",
    "  \n",
    "def import_cifar(dataset):\n",
    "    if dataset == 10:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    elif dataset == 100:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    # By default, they are uint8 but we need them float to normalize them\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # Calculating the mean and standard deviation of the training data\n",
    "    mean = np.mean(x_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(x_train, axis=(0, 1, 2, 3))\n",
    "    \n",
    "    # Normalizing \n",
    "    x_train = normalize(x_train, mean, std)\n",
    "    x_test = normalize(x_test, mean, std)\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes=dataset)\n",
    "    y_test = to_categorical(y_test,  num_classes=dataset)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 254523,
     "status": "ok",
     "timestamp": 1553629538981,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "WV_iymO6PAJP",
    "outputId": "c9f4bcdb-1ba5-4402-ac75-21cb49868b08"
   },
   "outputs": [],
   "source": [
    "# LOAD DATABase\n",
    "num_classes = 10\n",
    "(x_train, y_train), (x_test, y_test) = import_cifar(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trCmD1WVQlEv"
   },
   "outputs": [],
   "source": [
    "# Architecture taken from https://github.com/geifmany/cifar-vgg\n",
    "# Weight decay and Dropout have been removed\n",
    "# BatchNormalization before activations\n",
    "def VGG16_Vanilla_beta(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        #0\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #3\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #7\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #10\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #14\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #17\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #20\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #24\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #27\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #30\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #34\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #37\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #40\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        #45\n",
    "        Dense(512),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #48\n",
    "        Dense(num_classes),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('softmax')])\n",
    "    return model\n",
    "\n",
    "# Architecture taken from https://github.com/geifmany/cifar-vgg\n",
    "# BatchNormalization before activations\n",
    "def VGG16_beta(input_shape, num_classes, weight_decay):\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.3),\n",
    "        Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.4),\n",
    "        Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(512, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #Dropout(0.5),\n",
    "        Dense(num_classes, kernel_regularizer=regularizers.l2(weight_decay)),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('softmax')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'modelLAY_65.49%test_94%train_epoch50_0012lr.hdf5'\n",
    "\n",
    "modelename = 'vgg16_layca_1'\n",
    "#layca, sgd, wdecay\n",
    "model_name = '../weights/4/{}/final/weights-final.hdf5'.format(modelename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 345320,
     "status": "ok",
     "timestamp": 1553629629855,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "t9gnffFxQ8JJ",
    "outputId": "88226a4f-612d-4a6a-f3fb-facb256c5034"
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL SDG/LAYCA or WD\n",
    "model = VGG16_Vanilla_beta(input_shape=(32,32,3), num_classes=10)\n",
    "#model = VGG16_beta(input_shape=(32,32,3), num_classes=10, weight_decay=0.005)\n",
    "\n",
    "#model.load_weights('weights/{}.hdf5'.format(model_name))\n",
    "model.load_weights('{}'.format(model_name))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.002, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 377703,
     "status": "ok",
     "timestamp": 1553629662262,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "F3CL2zkoRGex",
    "outputId": "0b0eb53d-3225-4afa-b143-f474bffd9074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 371us/step\n",
      "Test loss: 1.0062860960006714\n",
      "Test accuracy: 0.7412\n",
      "25.88% : Model Error\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION\n",
    "# Final evaluation of the models\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "print(\"%.2f%% : Model Error\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cAGlHxHIRLJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv2d_4/BiasAdd:0', 'conv2d_11/BiasAdd:0', 'activation_5/Relu:0', 'conv2d_1/BiasAdd:0', 'batch_normalization_15/cond/Merge:0', 'max_pooling2d_3/MaxPool:0', 'activation_10/Relu:0', 'activation_4/Relu:0', 'activation_6/Relu:0', 'batch_normalization_4/cond/Merge:0', 'max_pooling2d_4/MaxPool:0', 'conv2d_2/BiasAdd:0', 'batch_normalization_1/cond/Merge:0', 'max_pooling2d_2/MaxPool:0', 'conv2d_12/BiasAdd:0', 'activation_9/Relu:0', 'activation_12/Relu:0', 'batch_normalization_7/cond/Merge:0', 'batch_normalization_11/cond/Merge:0', 'activation_3/Relu:0', 'conv2d_3/BiasAdd:0', 'activation_8/Relu:0', 'batch_normalization_5/cond/Merge:0', 'batch_normalization_10/cond/Merge:0', 'conv2d_13/BiasAdd:0', 'conv2d_6/BiasAdd:0', 'batch_normalization_12/cond/Merge:0', 'conv2d_5/BiasAdd:0', 'activation_11/Relu:0', 'conv2d_7/BiasAdd:0', 'activation_7/Relu:0', 'flatten_1/Reshape:0', 'activation_14/Relu:0', 'dense_2/BiasAdd:0', 'activation_1/Relu:0', 'conv2d_10/BiasAdd:0', 'conv2d_9/BiasAdd:0', 'batch_normalization_6/cond/Merge:0', 'batch_normalization_13/cond/Merge:0', 'batch_normalization_8/cond/Merge:0', 'batch_normalization_14/cond/Merge:0', 'batch_normalization_3/cond/Merge:0', 'conv2d_8/BiasAdd:0', 'batch_normalization_2/cond/Merge:0', 'batch_normalization_9/cond/Merge:0', 'activation_13/Relu:0', 'max_pooling2d_5/MaxPool:0', 'activation_15/Softmax:0', 'dense_1/BiasAdd:0', 'max_pooling2d_1/MaxPool:0', 'activation_2/Relu:0'])\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               1024      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 14,991,966\n",
      "Trainable params: 14,982,474\n",
      "Non-trainable params: 9,492\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# On regarde l'activation des pairs de neurones pour un echantillon test pris au hasard\n",
    "Echantillon = []\n",
    "nombreDImagesDActivation = 1000\n",
    "choix = np.random.choice(x_test.shape[0], nombreDImagesDActivation)\n",
    "Echantillon = x_train[choix, :, : , :]\n",
    "\n",
    "IM = get_activations(model, Echantillon)\n",
    "# on affiche les noms des differentes couches\n",
    "print(IM.keys())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we suppose k = 2 (activé and inactivé)\n",
    "\n",
    "def AverageDistance(vecteur,vecteurs_without_vecteur):\n",
    "    nCluster_i = len(vecteurs_without_vecteur)\n",
    "    distance = 0\n",
    "    for vect in vecteurs_without_vecteur:\n",
    "        #distance = distance + scipy.spatial.distance.euclidean(vecteur,vect)\n",
    "        distance = distance + numpy.linalg.norm(vecteur-vect)\n",
    "    a = (1/(nCluster_i))*distance\n",
    "    return a\n",
    "\n",
    "def AverageDissimilarity(vecteur, vecteurs_from_another_cluster):\n",
    "    nCluster_j = len(vecteurs_from_another_cluster)\n",
    "    distance = 0\n",
    "    for vect in vecteurs_from_another_cluster:\n",
    "        #distance = distance + scipy.spatial.distance.euclidean(vecteur,vect)\n",
    "        distance = distance + numpy.linalg.norm(vecteur-vect)\n",
    "    b = (1/nCluster_j)*distance\n",
    "    return b\n",
    "\n",
    "def Silhouette(vecteurs, a, b):\n",
    "    s = (b-a)/(max(a,b))\n",
    "    return s\n",
    "\n",
    "# quid si pas assez de partité ?\n",
    "def SilhouetteScore(vecteurs_0,vecteurs_1):\n",
    "    if len(vecteurs_0) == None or len(vecteurs_1) == None or len(vecteurs_0) == 0 or len(vecteurs_1) == 0:\n",
    "        print('Error: all entries actived or not: Probably a pathologique neuron')\n",
    "        return -2\n",
    "    \n",
    "    SilhouetteScoreSum = 0\n",
    "    \n",
    "    if len(vecteurs_0) < 3 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 1 #0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_0)):\n",
    "            a = AverageDistance(vecteurs_0[i], vecteurs_0[0:i]+vecteurs_0[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_0[i], vecteurs_1)\n",
    "            s = Silhouette(vecteurs_0, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "    \n",
    "    if len(vecteurs_1) < 2 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 1 #0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_1)):\n",
    "            a = AverageDistance(vecteurs_1[i], vecteurs_1[0:i]+vecteurs_1[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_1[i], vecteurs_0)\n",
    "            s = Silhouette(vecteurs_1, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "\n",
    "    SilhouetteScore = SilhouetteScoreSum/(len(vecteurs_0)+len(vecteurs_1))\n",
    "        \n",
    "    return SilhouetteScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TakeTheNearest(Vector, Ratio):\n",
    "    Length = len(Vector)\n",
    "    DistVector = []\n",
    "    for i in range(len(Vector)):\n",
    "        Dist = 0\n",
    "        for j in range(len(Vector)):\n",
    "            if i!=j:\n",
    "                Dist = Dist + scipy.spatial.distance.euclidean(Vector[i],Vector[j])\n",
    "        #'cosine'\n",
    "        DistVector.append(Dist)\n",
    "    #print(len(Vector[0])) # nombre de dimension des activations\n",
    "    #print(len(DistVector)) # nombre d'activations activé ou non-activé\n",
    "    \n",
    "    UltimeVector = zip(DistVector, Vector)\n",
    "    UltimeVectorList = list(UltimeVector)\n",
    "    #print(UltimeVectorList)\n",
    "    #print(UltimeVectorList[0][0]) # 1ere distance totale\n",
    "    #print(UltimeVectorList[0][1]) # coordonnées de la premiere activation\n",
    "    #print(UltimeVectorList[:][1]) # toutes les coordonnées\n",
    "    UltimeVectorList.sort(key=operator.itemgetter(0))\n",
    "    #print(UltimeVectorList)\n",
    "    #print(UltimeVectorList[0][0]) # distance 0\n",
    "    #print(UltimeVectorList[0][1]) # coord 0\n",
    "    #print(UltimeVectorList[1][0]) # dist 1\n",
    "    \n",
    "    Vec = [lis[1] for lis in UltimeVectorList]\n",
    "    NewVector = Vec[:int(np.floor(Length*Ratio))]\n",
    "    return NewVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette score mean by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RUN(Ratio, NeuronsToTest):\n",
    "    Silhouette_Score_Mean = []\n",
    "    NbreOfNeurons = []\n",
    "    for c in range(12):\n",
    "        print('Couche '+str(c))\n",
    "        #Layer to test\n",
    "        couche_name_1 = 'activation_{}/Relu:0'.format(c+1)\n",
    "        couche_name_2 = 'conv2d_{}/BiasAdd:0'.format(c+2)\n",
    "        couche_name_3 = 'activation_{}/Relu:0'.format(c+2)\n",
    "\n",
    "        NeuronsToTestIntoTheLayer = NeuronsToTest\n",
    "\n",
    "        X_Kernel_size = len(IM[couche_name_2][0,:,:,:])\n",
    "        Y_Kernel_size = len(IM[couche_name_2][0,0,:,:])\n",
    "        Z_Kernel_size = len(IM[couche_name_2][0,0,0,:])\n",
    "\n",
    "        Silhouette_Score_Sum = 0\n",
    "\n",
    "        for i in range(NeuronsToTestIntoTheLayer):\n",
    "            print('Neurone '+str(i))\n",
    "\n",
    "            #Choix du neurone dans la couche\n",
    "            X = int(numpy.ceil(numpy.random.rand(1)*X_Kernel_size)-1)\n",
    "            Y = int(numpy.ceil(numpy.random.rand(1)*Y_Kernel_size)-1)\n",
    "            Z = int(numpy.ceil(numpy.random.rand(1)*Z_Kernel_size)-1)\n",
    "            #specification de ses bornes\n",
    "            X_inf = X-1\n",
    "            Y_inf = Y-1\n",
    "            X_sup = X+1\n",
    "            Y_sup = Y+1\n",
    "            if X == 0: X_inf = 0\n",
    "            if Y == 0: Y_inf = 0   \n",
    "            if X == X_Kernel_size-1: X_sup = X_Kernel_size-1\n",
    "            if Y == Y_Kernel_size-1: Y_sup = Y_Kernel_size-1\n",
    "\n",
    "            Vecteurs_0 = [] #inactivé\n",
    "            Vecteurs_1 = [] #activé\n",
    "            ##Vecteurs_All = []\n",
    "            ##Labels_All = []\n",
    "\n",
    "            for j in range(nombreDImagesDActivation):\n",
    "                if IM[couche_name_3][j,X,Y,Z] == 0.0:\n",
    "                    Vecteurs_0.append(IM[couche_name_1][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                    ##Vecteurs_All.append(IM[couche_name_1][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                    ##Labels_All.append(0)\n",
    "                elif IM[couche_name_3][j,X,Y,Z] > 0.0:\n",
    "                    Vecteurs_1.append(IM[couche_name_1][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                    ##Vecteurs_All.append(IM[couche_name_1][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                    ##Labels_All.append(1)\n",
    "                else:\n",
    "                    print('error !')\n",
    "                    break\n",
    "\n",
    "            Vecteurs_0 = TakeTheNearest(Vecteurs_0, Ratio)\n",
    "            Vecteurs_1 = TakeTheNearest(Vecteurs_1, Ratio)\n",
    "\n",
    "            S = SilhouetteScore(Vecteurs_0,Vecteurs_1)\n",
    "            ##S2 = sklearn.metrics.silhouette_score(Vecteurs_All, Labels_All, metric='euclidean') # -> give the same result ! :)\n",
    "\n",
    "            if S != -2:\n",
    "                #print('Neuron '+str(i+1)+'/'+str(NeuronsToTestIntoTheLayer)+' has a silhouette score of '+str(S))\n",
    "                Silhouette_Score_Sum = Silhouette_Score_Sum + S\n",
    "            else:\n",
    "                NeuronsToTestIntoTheLayer = NeuronsToTestIntoTheLayer -1\n",
    "\n",
    "        Silhouette_Score_Mean.append(Silhouette_Score_Sum / NeuronsToTestIntoTheLayer)\n",
    "        NbreOfNeurons.append(NeuronsToTestIntoTheLayer)\n",
    "    print(Silhouette_Score_Mean)\n",
    "    print(np.mean(Silhouette_Score_Mean))\n",
    "    \n",
    "    # plot:\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.ylim(top=1)\n",
    "    nombreCouches = len(Silhouette_Score_Mean)\n",
    "    x = range(nombreCouches)\n",
    "\n",
    "    markerline, stemlines, baseline = plt.stem(x, Silhouette_Score_Mean, markerfmt='o', label='model ')\n",
    "    plt.setp(stemlines, 'color', plt.getp(markerline,'color'))\n",
    "    plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "    plt.plot(np.mean(Silhouette_Score_Mean)*np.ones((nombreCouches,1)))\n",
    "    for c in range(12):\n",
    "        plt.text(-0.27+c, 0.02, NbreOfNeurons[c], rotation= 90)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Couches')\n",
    "    plt.xticks(np.arange(nombreCouches), ('Conv 1','Conv 2','Conv 3','Conv 4','Conv 5','Conv 6','Conv 7','Conv 8','Conv 9','Conv10','Conv11','Conv12','Conv13','Dense1','Dense2'), rotation=60)\n",
    "    plt.ylabel('Silhouette score mean')\n",
    "    plt.title('Silhouette score mean for the {} by layer for the model {}\\n for {} activation images and {} neurons'.format(Ratio,modelename,nombreDImagesDActivation,NeuronsToTest))\n",
    "    plt.show()\n",
    "    fig.savefig('figures/clustering/SilhouetteScoreMean_{}_Ratio{}.png'.format(modelename, Ratio), dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "Couche 0\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 1\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 2\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 3\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 4\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 5\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 6\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 7\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 8\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n",
      "Couche 9\n",
      "Neurone 0\n",
      "Neurone 1\n",
      "Neurone 2\n",
      "Neurone 3\n",
      "Neurone 4\n",
      "Neurone 5\n",
      "Neurone 6\n",
      "Neurone 7\n",
      "Neurone 8\n",
      "Neurone 9\n"
     ]
    }
   ],
   "source": [
    "for j in range(5):\n",
    "    ratio = round(0.05+0.2*np.round(j), 2)\n",
    "    print(ratio)\n",
    "    RUN(Ratio=ratio, NeuronsToTest=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "V5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
