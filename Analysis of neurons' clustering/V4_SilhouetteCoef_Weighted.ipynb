{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul le coefficient de silhouette pour une distance cosine/euclidean pour les X% d'activés les plus proches entre eux, en regardant les 10% les plus proches (activés/non activés pour calculer le coef)\n",
    "\n",
    "Temps environ: 20h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#IPython extension to reload modules before executing user code.\n",
    "#'autoreload' reloads modules automatically before entering the execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#set memory usage to 0.5\\nfrom keras.backend.tensorflow_backend import set_session\\nimport tensorflow as tf\\nconfig = tf.ConfigProto()\\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.5\\nset_session(tf.Session(config=config))'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select the best free GPU on the nvidia card\n",
    "stats = gpustat.GPUStatCollection.new_query()\n",
    "ids = map(lambda gpu: int(gpu.entry['index']), stats)\n",
    "ratios = map(lambda gpu: float(gpu.entry['memory.used'])/float(gpu.entry['memory.total']), stats)\n",
    "bestGPU = min(zip(ids, ratios), key=lambda x: x[1])[0]\n",
    "bestGPU = 3\n",
    "\n",
    "print(\"setGPU: Setting GPU to: {}\".format(bestGPU))\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(bestGPU)\n",
    "\n",
    "'''\n",
    "#set memory usage to 0.5\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7754,
     "status": "ok",
     "timestamp": 1553629292171,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "WqsDDrUyOQbM",
    "outputId": "3a0b30e9-8797-4d08-b497-2ecd3eec3c59",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q sklearn\n",
    "import collections\n",
    "import numpy as np, numpy\n",
    "from keract import get_activations, display_activations\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.datasets import cifar10, cifar100 # we can use also cifar100\n",
    "from keras.layers import Input, BatchNormalization, AveragePooling2D, ZeroPadding2D, LeakyReLU, GlobalAveragePooling2D, Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import operator \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sys\n",
    "#sys.executable\n",
    "#sys.path\n",
    "import time\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbtqLzrROuSj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(x, mean, std):\n",
    "    # This function normalizes inputs for zero mean and unit variance to speed up learning.\n",
    "    \n",
    "    # In case std = 0, we add eps = 1e-7\n",
    "    eps = K.epsilon()\n",
    "    x = (x-mean)/(std+eps)\n",
    "    return x\n",
    "  \n",
    "def import_cifar(dataset):\n",
    "    if dataset == 10:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    elif dataset == 100:\n",
    "        (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    # By default, they are uint8 but we need them float to normalize them\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    \n",
    "    # Calculating the mean and standard deviation of the training data\n",
    "    mean = np.mean(x_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(x_train, axis=(0, 1, 2, 3))\n",
    "    \n",
    "    # Normalizing \n",
    "    x_train = normalize(x_train, mean, std)\n",
    "    x_test = normalize(x_test, mean, std)\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes=dataset)\n",
    "    y_test = to_categorical(y_test,  num_classes=dataset)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 254523,
     "status": "ok",
     "timestamp": 1553629538981,
     "user": {
      "displayName": "Antoine Van Hoof",
      "photoUrl": "",
      "userId": "14540243825078360446"
     },
     "user_tz": -60
    },
    "id": "WV_iymO6PAJP",
    "outputId": "c9f4bcdb-1ba5-4402-ac75-21cb49868b08",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATABase\n",
    "num_classes = 10\n",
    "(x_train, y_train), (x_test, y_test) = import_cifar(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trCmD1WVQlEv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Architecture taken from https://github.com/geifmany/cifar-vgg\n",
    "# Weight decay and Dropout have been removed\n",
    "# BatchNormalization before activations\n",
    "def VGG16_Vanilla_beta(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        #0\n",
    "        Conv2D(64, (3, 3), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #3\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #7\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #10\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #14\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #17\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #20\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #24\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #27\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #30\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        #34\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #37\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #40\n",
    "        Conv2D(512, (3, 3), padding='same'),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        #45\n",
    "        Dense(512),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('relu'),\n",
    "        #48\n",
    "        Dense(num_classes),\n",
    "        BatchNormalization(scale=False, center=False),\n",
    "        Activation('softmax')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we suppose k = 2 (activé and inactivé)\n",
    "\n",
    "def AverageDistance(vecteur,vecteurs_without_vecteur):\n",
    "    nCluster_i = len(vecteurs_without_vecteur)\n",
    "    distance = 0\n",
    "    for vect in vecteurs_without_vecteur:\n",
    "        #distance = distance + scipy.spatial.distance.euclidean(vecteur,vect)\n",
    "        distance = distance + numpy.linalg.norm(vecteur-vect)\n",
    "    a = (1/(nCluster_i))*distance\n",
    "    return a\n",
    "\n",
    "def AverageDissimilarity(vecteur, vecteurs_from_another_cluster):\n",
    "    nCluster_j = len(vecteurs_from_another_cluster)\n",
    "    distance = 0\n",
    "    for vect in vecteurs_from_another_cluster:\n",
    "        #distance = distance + scipy.spatial.distance.euclidean(vecteur,vect)\n",
    "        distance = distance + numpy.linalg.norm(vecteur-vect)\n",
    "    b = (1/nCluster_j)*distance\n",
    "    return b\n",
    "\n",
    "def Silhouette(vecteurs, a, b):\n",
    "    s = (b-a)/(max(a,b))\n",
    "    return s\n",
    "\n",
    "# quid si pas assez de partité ?\n",
    "def SilhouetteScore(vecteurs_0,vecteurs_1):\n",
    "    if len(vecteurs_0) == None or len(vecteurs_1) == None or len(vecteurs_0) == 0 or len(vecteurs_1) == 0:\n",
    "        print('Error: all entries actived or not: Probably a pathologique neuron')\n",
    "        return -2\n",
    "    \n",
    "    SilhouetteScoreSum = 0\n",
    "    \n",
    "    if len(vecteurs_0) < 3 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 1 #0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_0)):\n",
    "            a = AverageDistance(vecteurs_0[i], vecteurs_0[0:i]+vecteurs_0[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_0[i], vecteurs_1)\n",
    "            s = Silhouette(vecteurs_0, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "    \n",
    "    if len(vecteurs_1) < 2 :\n",
    "        SilhouetteScoreSum = SilhouetteScoreSum + 1 #0\n",
    "    else:\n",
    "        for i in range(len(vecteurs_1)):\n",
    "            a = AverageDistance(vecteurs_1[i], vecteurs_1[0:i]+vecteurs_1[(i+1):])\n",
    "            b = AverageDissimilarity(vecteurs_1[i], vecteurs_0)\n",
    "            s = Silhouette(vecteurs_1, a, b)\n",
    "            SilhouetteScoreSum = SilhouetteScoreSum + s\n",
    "\n",
    "    SilhouetteScore = SilhouetteScoreSum/(len(vecteurs_0)+len(vecteurs_1))\n",
    "        \n",
    "    return SilhouetteScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def TakeTheNearest(Vector, Ratio, Dist):\n",
    "    Length = len(Vector)\n",
    "    DistVector = []\n",
    "    if Dist == 'cosine':\n",
    "        for i in range(len(Vector)):\n",
    "            Dist = 0\n",
    "            for j in range(len(Vector)):\n",
    "                if i!=j:\n",
    "                    Dist = Dist + scipy.spatial.distance.cosine(Vector[i],Vector[j])\n",
    "            DistVector.append(Dist)\n",
    "    elif Dist == 'euclidean':\n",
    "        for i in range(len(Vector)):\n",
    "            Dist = 0\n",
    "            for j in range(len(Vector)):\n",
    "                if i!=j:\n",
    "                    Dist = Dist + scipy.spatial.distance.euclidean(Vector[i],Vector[j])\n",
    "            DistVector.append(Dist)\n",
    "    else:\n",
    "        print(\"ERROR in the Distance specification\")\n",
    "    #print(len(Vector[0])) # nombre de dimension des activations\n",
    "    #print(len(DistVector)) # nombre d'activations activé ou non-activé\n",
    "    \n",
    "    UltimeVector = zip(DistVector, Vector)\n",
    "    UltimeVectorList = list(UltimeVector)\n",
    "    #print(UltimeVectorList)\n",
    "    #print(UltimeVectorList[0][0]) # 1ere distance totale\n",
    "    #print(UltimeVectorList[0][1]) # coordonnées de la premiere activation\n",
    "    #print(UltimeVectorList[:][1]) # toutes les coordonnées\n",
    "    UltimeVectorList.sort(key=operator.itemgetter(0))\n",
    "    #print(UltimeVectorList)\n",
    "    #print(UltimeVectorList[0][0]) # distance 0\n",
    "    #print(UltimeVectorList[0][1]) # coord 0\n",
    "    #print(UltimeVectorList[1][0]) # dist 1\n",
    "    \n",
    "    Vec = [lis[1] for lis in UltimeVectorList]\n",
    "    NewVector = Vec[:int(np.floor(Length*Ratio))]\n",
    "    return NewVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def CheckTheNearests(point, Vecteur, Label, Ratio, Dist):\n",
    "    point = numpy.asarray([point])\n",
    "    Vecteur = numpy.asarray(Vecteur)\n",
    "    Length = len(Vecteur)\n",
    "    Distances = scipy.spatial.distance.cdist(point, Vecteur, metric=Dist) #return a matrix\n",
    "    \n",
    "    UltimeVector = zip(Distances[0], Vecteur, Label)\n",
    "    UltimeVectorList = list(UltimeVector)\n",
    "    UltimeVectorList.sort(key=operator.itemgetter(0))\n",
    "    Vec = [lis[1] for lis in UltimeVectorList]\n",
    "    Vect = Vec[:int(np.floor(Length*Ratio))]\n",
    "    Vec = [lis[2] for lis in UltimeVectorList]\n",
    "    Lab = Vec[:int(np.floor(Length*Ratio))]\n",
    "\n",
    "    return Vect, Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette score mean by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def RUN(IM, modelename, Dist, Ratio1, Ratio2, NeuronsToTest):\n",
    "    Silhouette_Score_Mean = []\n",
    "    NbreOfNeurons = []\n",
    "    NumberOfLayer = 13\n",
    "    labelRatioMean = []\n",
    "    for c in range(NumberOfLayer):\n",
    "        print('\\nCouche '+str(c+1))\n",
    "        #Layer to test\n",
    "        couche_name_2 = 'conv2d_{}/BiasAdd:0'.format(c+1)\n",
    "        couche_name_3 = 'activation_{}/Relu:0'.format(c+1)\n",
    "\n",
    "        NeuronsToTestIntoTheLayer = NeuronsToTest\n",
    "\n",
    "        X_Kernel_size = len(IM[couche_name_2][0,:,:,:])\n",
    "        Y_Kernel_size = len(IM[couche_name_2][0,0,:,:])\n",
    "        Z_Kernel_size = len(IM[couche_name_2][0,0,0,:])\n",
    "\n",
    "        Silhouette_Score_Sum = 0\n",
    "        labelRatio = []\n",
    "\n",
    "        for i in range(NeuronsToTestIntoTheLayer):\n",
    "            print(' Neurone '+str(i))\n",
    "\n",
    "            #Choix du neurone dans la couche\n",
    "            X = int(numpy.ceil(numpy.random.rand(1)*X_Kernel_size)-1)\n",
    "            Y = int(numpy.ceil(numpy.random.rand(1)*Y_Kernel_size)-1)\n",
    "            Z = int(numpy.ceil(numpy.random.rand(1)*Z_Kernel_size)-1)\n",
    "            #specification de ses bornes\n",
    "            X_inf = X-1\n",
    "            Y_inf = Y-1\n",
    "            X_sup = X+1\n",
    "            Y_sup = Y+1\n",
    "            if X == 0: X_inf = 0\n",
    "            if Y == 0: Y_inf = 0   \n",
    "            if X == X_Kernel_size-1: X_sup = X_Kernel_size-1\n",
    "            if Y == Y_Kernel_size-1: Y_sup = Y_Kernel_size-1\n",
    "\n",
    "            Vecteurs_1 = [] #elements activés\n",
    "            Vecteurs_All = [] #tous\n",
    "            Labels_All = [] #leur label\n",
    "\n",
    "            for j in range(nombreDImagesDActivation):\n",
    "                if IM[couche_name_3][j,X,Y,Z] == 0.0:\n",
    "                    Vecteurs_All.append(IM[couche_name_2][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                    Labels_All.append(0)\n",
    "                elif IM[couche_name_3][j,X,Y,Z] > 0.0:\n",
    "                    Vecteurs_1.append(IM[couche_name_2][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                    Vecteurs_All.append(IM[couche_name_2][j,X_inf:X_sup,Y_inf:Y_sup,0:Z_Kernel_size].flatten())\n",
    "                    Labels_All.append(1)\n",
    "\n",
    "            Vecteurs_1_nearest = TakeTheNearest(Vecteurs_1, Ratio1, Dist)\n",
    "                \n",
    "            #pour chaque element de Vecteurs_1_nearest\n",
    "            Length_Vecteurs_1_nearest = len(Vecteurs_1_nearest)\n",
    "            print('   Number of {} ration nearest activated element :'.format(Ratio1)+str(Length_Vecteurs_1_nearest))\n",
    "            if Length_Vecteurs_1_nearest == 0:\n",
    "                NeuronsToTestIntoTheLayer = NeuronsToTestIntoTheLayer -1\n",
    "            else:\n",
    "                labelUniqueError = 0\n",
    "                S_Sum = 0\n",
    "                for k in range(Length_Vecteurs_1_nearest):\n",
    "                    Vect, Lab = CheckTheNearests(Vecteurs_1_nearest[k], Vecteurs_All, Labels_All, Ratio2, Dist)\n",
    "                    #assert if there are strictly more than 1 label\n",
    "                    if len(set(Lab)) >1:\n",
    "                        S = sklearn.metrics.silhouette_score(Vect, Lab, metric=Dist)\n",
    "                    else:\n",
    "                        S = -2\n",
    "\n",
    "                    if S != -2:\n",
    "                        S_Sum = S_Sum + S\n",
    "                    else:\n",
    "                        labelUniqueError = labelUniqueError + 1\n",
    "                    \n",
    "                    # count the ratio of activated label around the activated point\n",
    "                    labelRatio.append(Lab.count(1)/len(Lab))\n",
    "                    #print('        Label activated ratio: '+str(labelRatio[-1]))\n",
    "                \n",
    "                labelErrorRatio = labelUniqueError/Length_Vecteurs_1_nearest\n",
    "                print('   Label Unique Error into the '+str(Ratio2)+' ratio activated point area Ratio:'+str(labelErrorRatio))\n",
    "        \n",
    "                if labelUniqueError != Length_Vecteurs_1_nearest:\n",
    "                    Silhouette_Score_Sum = Silhouette_Score_Sum + S_Sum/(Length_Vecteurs_1_nearest - labelUniqueError)\n",
    "                else: \n",
    "                    NeuronsToTestIntoTheLayer = NeuronsToTestIntoTheLayer -1\n",
    "                \n",
    "        if NeuronsToTestIntoTheLayer != 0:\n",
    "            Silhouette_Score_Mean.append(Silhouette_Score_Sum / NeuronsToTestIntoTheLayer)\n",
    "            print(' => Silhouette_Score_Mean : '+str(Silhouette_Score_Mean[-1]))\n",
    "        else:\n",
    "            Silhouette_Score_Mean.append(0)\n",
    "            print('  !!!  no neurons to test into this layer, only one label (activated or not)')\n",
    "        NbreOfNeurons.append(NeuronsToTestIntoTheLayer)\n",
    "        \n",
    "        labelRatioMean.append(np.mean(labelRatio))\n",
    "        \n",
    "    print(Silhouette_Score_Mean)\n",
    "    print(np.mean(Silhouette_Score_Mean))\n",
    "    \n",
    "    # plot:\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.ylim(top=1)\n",
    "    nombreCouches = len(Silhouette_Score_Mean)\n",
    "    x = range(nombreCouches)\n",
    "\n",
    "    markerline, stemlines, baseline = plt.stem(x, Silhouette_Score_Mean, markerfmt='o', label='model ')\n",
    "    plt.setp(stemlines, 'color', plt.getp(markerline,'color'))\n",
    "    plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "    plt.plot(np.mean(Silhouette_Score_Mean)*np.ones((nombreCouches,1)))\n",
    "    for c in range(NumberOfLayer):\n",
    "        plt.text(-0.27+c, 0.02, NbreOfNeurons[c], rotation= 90)\n",
    "\n",
    "    #plt.legend()\n",
    "    #plt.xlabel('Couches')\n",
    "    #plt.xticks(np.arange(nombreCouches), ('Conv 1','Conv 2','Conv 3','Conv 4','Conv 5','Conv 6','Conv 7','Conv 8','Conv 9','Conv10','Conv11','Conv12','Conv13'), rotation=60)\n",
    "    #plt.ylabel('Silhouette coef mean')\n",
    "    #plt.title('Silhouette coef mean by layer for the model {}\\n for {} activation images and {} neurons \\n with Ratio1 {} and Ratio2 {}'.format(modelename,nombreDImagesDActivation,NeuronsToTest, Ratio1, Ratio2))\n",
    "    plt.grid()\n",
    "    #plt.rcParams.update({'font.size': 20})\n",
    "    plt.show()\n",
    "    fig.savefig('figures/SilhouetteCoef_Weighted/SilhouetteCoefMean_{}_Ratios{}_{}_{}neurons_{}.png'.format(modelename, Ratio1, Ratio2,NeuronsToTest, Dist), dpi=100) \n",
    "    \n",
    "    \n",
    "    # plot2 label ratio:\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.ylim(top=1)\n",
    "    nombreCouches = len(Silhouette_Score_Mean)\n",
    "    x = range(nombreCouches)\n",
    "\n",
    "    markerline, stemlines, baseline = plt.stem(x, labelRatioMean, markerfmt='o', label='model ')\n",
    "    plt.setp(stemlines, 'color', plt.getp(markerline,'color'))\n",
    "    plt.setp(stemlines, 'linestyle', 'dotted')\n",
    "    plt.plot(np.mean(labelRatioMean)*np.ones((nombreCouches,1)))\n",
    "    #plt.rcParams.update({'font.size': 20})\n",
    "    plt.grid()\n",
    "    #plt.xticks(np.arange(nombreCouches), ('Conv 1','Conv 2','Conv 3','Conv 4','Conv 5','Conv 6','Conv 7','Conv 8','Conv 9','Conv10','Conv11','Conv12','Conv13'), rotation=60)\n",
    "    #plt.ylabel('label Ratio Mean')\n",
    "    #plt.title('label Ratio Mean by layer for the model {}\\n for {} activation images and {} neurons \\n with Ratio1 {} and Ratio2 {}'.format(modelename,nombreDImagesDActivation,NeuronsToTest, Ratio1, Ratio2))\n",
    "    plt.show()\n",
    "    fig.savefig('figures/SilhouetteCoef_Weighted/labelRatioMean_{}_Ratios{}_{}_{}neurons_{}.png'.format(modelename, Ratio1, Ratio2,NeuronsToTest, Dist), dpi=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD the model and get the activations\n",
    "modelename = 'vgg16_wdecay_0'\n",
    "model_name = '../weights/6/{}/initial/weights-initial.hdf5'.format(modelename) #final / initial ! attention au nom de sauvegarde !\n",
    "modelename = 'initial'\n",
    "model = VGG16_Vanilla_beta(input_shape=(32,32,3), num_classes=10)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "sgd = optimizers.SGD(lr=0.002, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "Echantillon = []\n",
    "nombreDImagesDActivation = 1000\n",
    "choix = np.random.choice(x_test.shape[0], nombreDImagesDActivation)\n",
    "Echantillon = x_train[choix, :, : , :]\n",
    "IM = get_activations(model, Echantillon)# on affiche les noms des differentes couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['activation_21/Relu:0', 'conv2d_15/BiasAdd:0', 'batch_normalization_25/cond/Merge:0', 'activation_22/Relu:0', 'conv2d_19/BiasAdd:0', 'activation_28/Relu:0', 'conv2d_14/BiasAdd:0', 'conv2d_20/BiasAdd:0', 'batch_normalization_29/cond/Merge:0', 'activation_18/Relu:0', 'batch_normalization_21/cond/Merge:0', 'batch_normalization_19/cond/Merge:0', 'dense_3/BiasAdd:0', 'activation_26/Relu:0', 'batch_normalization_26/cond/Merge:0', 'flatten_2/Reshape:0', 'conv2d_25/BiasAdd:0', 'activation_20/Relu:0', 'activation_30/Softmax:0', 'batch_normalization_30/cond/Merge:0', 'conv2d_17/BiasAdd:0', 'conv2d_26/BiasAdd:0', 'batch_normalization_18/cond/Merge:0', 'batch_normalization_27/cond/Merge:0', 'max_pooling2d_7/MaxPool:0', 'activation_23/Relu:0', 'activation_27/Relu:0', 'conv2d_18/BiasAdd:0', 'batch_normalization_24/cond/Merge:0', 'max_pooling2d_10/MaxPool:0', 'conv2d_21/BiasAdd:0', 'max_pooling2d_8/MaxPool:0', 'batch_normalization_17/cond/Merge:0', 'activation_19/Relu:0', 'batch_normalization_28/cond/Merge:0', 'activation_24/Relu:0', 'conv2d_16/BiasAdd:0', 'batch_normalization_23/cond/Merge:0', 'conv2d_23/BiasAdd:0', 'activation_29/Relu:0', 'batch_normalization_20/cond/Merge:0', 'conv2d_24/BiasAdd:0', 'batch_normalization_16/cond/Merge:0', 'activation_25/Relu:0', 'conv2d_22/BiasAdd:0', 'activation_16/Relu:0', 'max_pooling2d_6/MaxPool:0', 'max_pooling2d_9/MaxPool:0', 'dense_4/BiasAdd:0', 'activation_17/Relu:0', 'batch_normalization_22/cond/Merge:0'])\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 16, 16, 128)       256       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 16, 16, 128)       256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 8, 8, 256)         512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 4, 4, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 2, 2, 512)         1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 512)               1024      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 14,991,966\n",
      "Trainable params: 14,982,474\n",
      "Non-trainable params: 9,492\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print the activations infos\n",
    "print(IM.keys())\n",
    "model.summary()\n",
    "## need to pick the ones that are weighted\n",
    "#print(IM['conv2d_1/BiasAdd:0'][0,0,0,:]) #activation, X, Y, Z\n",
    "#print(IM['batch_normalization_1/cond/Merge:0'][0,0,0,:]) #activation, X, Y, Z\n",
    "#print(IM['activation_1/Relu:0'][0,0,0,:]) #activation, X, Y, Z\n",
    "#print(IM['conv2d_2/BiasAdd:0'][0,0,0,:]) #activation, X, Y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RATIO1 = 0.05\n",
    "RATIO2 = 0.05\n",
    "NEURONSTOTEST = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the code\n",
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST) # diminuer de moitié le temps estimer si ok 10 neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_layca_0'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_layca_1'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_layca_2'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_sgd_0'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_sgd_1'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_sgd_2'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_wdecay_0'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_wdecay_1'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelename = 'vgg16_wdecay_2'\n",
    "model_name = '../weights/6/{}/final/weights-final.hdf5'.format(modelename)\n",
    "model.load_weights('{}'.format(model_name))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "IM = get_activations(model, Echantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='cosine', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN(IM, modelename, Dist='euclidean', Ratio1=RATIO1, Ratio2=RATIO2, NeuronsToTest=NEURONSTOTEST)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "V5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
